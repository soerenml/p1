{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SP500.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTLC1pZ/8654zh3YeADwGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soerenml/p1/blob/master/SP500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhGIu8BTynur"
      },
      "source": [
        "%%capture\n",
        "pip install \\\n",
        "yfinance \\\n",
        "apache-beam \\\n",
        "tensorflow \\\n",
        "tensorflow_transform"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx99G6vjzkwc"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymi3L4Y6zk5K"
      },
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "msft = yf.Ticker(\"MSFT\")\n",
        "hist = msft.history(period=\"max\")\n",
        "hist.to_csv('./hist.csv', index=False, columns=['Open','Volume'], header=False)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9rArzWa1dYm"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD_c6GjE5855"
      },
      "source": [
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'Open',\n",
        "    'Volume',\n",
        "]\n",
        "\n",
        "ORDERED_CSV_COLUMNS = [\n",
        "    'Open', 'Volume'\n",
        "]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piIxYibuIX7U"
      },
      "source": [
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] \n",
        ")\n",
        "\n",
        "SCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "    tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)).schema"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0hMh_8qIe0g"
      },
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
        "\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8VMPcjIyai"
      },
      "source": [
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a TFXIO to read the census data with the schema. To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "      # accepts a PCollection[bytes] because we need to patch the records first\n",
        "      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "      # to both read the CSV files and parse them to TFT inputs:\n",
        "      # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "      csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "          physical_format='text',\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "\n",
        "      # Read in raw data and convert using CSV TFXIO.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do the from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV TFXIO can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(\n",
        "              train_data_file, coder=beam.coders.BytesCoder())\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(b', ', b','))\n",
        "          | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n",
        "              preprocessing_fn, output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_data, _ = transformed_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestData' >> beam.io.ReadFromText(\n",
        "              test_data_file, skip_header_lines=2,\n",
        "              coder=beam.coders.BytesCoder())\n",
        "          | 'FixCommasTestData' >> beam.Map(\n",
        "              lambda line: line.replace(b', ', b','))\n",
        "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
        "          | 'DecodeTestData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn)\n",
        "          | tft_beam.TransformDataset(output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjjAMLVII3X7",
        "outputId": "81e60610-65d6-4970-9a39-0eaec391233e"
      },
      "source": [
        "from tfx_bsl.public import tfxio\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamples\n",
        "import os\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'\n",
        "\n",
        "transform_data('./hist.csv', './hist.csv', './')"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnRYls8dLWNK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}